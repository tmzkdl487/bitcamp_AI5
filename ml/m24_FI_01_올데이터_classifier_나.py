# 06_cancer
# 09_wine
# 11_digits

###  요 파일에 이 3개의 데이터셋 다 넣어서 23번처럼 맹그러.

from sklearn.datasets import load_iris, load_breast_cancer, load_wine, load_digits

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier

from sklearn.model_selection import train_test_split

#1. 데이터셋 로드
datasets = {
    "06_cancer": load_breast_cancer(return_X_y=True),
    "09_wine": load_wine(return_X_y=True),
    "11_digits": load_digits(return_X_y=True)
}

random_state = 1223

#2. 모델 리스트 정의
models = [
    DecisionTreeClassifier(random_state=random_state),
    RandomForestClassifier(random_state=random_state),
    GradientBoostingClassifier(random_state=random_state),
    XGBClassifier(random_state=random_state)
]

#3. 각 데이터셋에 대해 모델을 학습 및 평가
for name, (x, y) in datasets.items():
    print(f"### Dataset: {name} ###")
    
    x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, 
                                                        random_state=random_state, 
                                                        stratify=y)
    
    for model in models:
        model.fit(x_train, y_train)
        print("===================", model.__class__.__name__, "=====================")
        print('acc', model.score(x_test, y_test))
        print(model.feature_importances_)
    print("\n")  # 각 데이터셋의 결과를 구분하기 위해 추가

# ### Dataset: 06_cancer ###
# =================== DecisionTreeClassifier =====================   
# acc 0.9473684210526315
# [0.         0.05030732 0.         0.         0.         0.
#  0.         0.         0.         0.0125215  0.         0.03023319 
#  0.         0.         0.         0.         0.00785663 0.
#  0.         0.         0.72931244 0.         0.0222546  0.01862569 
#  0.01611893 0.         0.         0.0955152  0.01725451 0.        ]
# =================== RandomForestClassifier =====================
# acc 0.9298245614035088
# [0.02086793 0.01067931 0.04449403 0.05946836 0.00319558 0.02670713
#  0.02937097 0.0699787  0.00244273 0.0020165  0.02515119 0.00295319
#  0.00331962 0.02345914 0.00409896 0.00361462 0.00490543 0.00386996
#  0.00322823 0.00348085 0.12409483 0.0202164  0.14903546 0.1282556
#  0.01500504 0.01531022 0.03516301 0.15135923 0.0075333  0.00672446]
# =================== GradientBoostingClassifier =====================
# acc 0.9385964912280702
# [1.94023511e-04 2.49452840e-02 7.39095658e-04 1.11882290e-03
#  0.00000000e+00 5.15800222e-06 1.45512274e-02 4.88880805e-02
#  2.04936871e-05 2.09002025e-03 5.80367261e-04 8.49565095e-03
#  2.76122363e-03 1.54209684e-03 1.23141673e-03 0.00000000e+00
#  1.15082117e-05 1.07917966e-04 7.78817409e-05 2.26872696e-03
#  5.44493032e-01 3.78660080e-02 1.65403261e-01 2.52000516e-02
#  5.89157185e-03 4.95288035e-05 5.15995307e-03 1.05303238e-01
#  9.20938168e-04 8.34215822e-05]
# =================== XGBClassifier =====================
# acc 0.9385964912280702
# [0.01410364 0.01792158 0.         0.         0.         0.01414043
#  0.00264063 0.05220545 0.00094232 0.00426078 0.00051078 0.01685394
#  0.         0.01108845 0.00405639 0.00049153 0.00128015 0.00403847
#  0.00186279 0.00101614 0.38543397 0.01304734 0.3068675  0.01161618
#  0.01369678 0.         0.01367813 0.08019859 0.0109833  0.01706486]

#### # DecisionTreeClassifier만 두번째 특성이 가장 중요하다고 판단하고 
# 나머지 3개 모델은 21번째 특성이 가장 중요하다고 판단함.

# ### Dataset: 09_wine ###
# =================== DecisionTreeClassifier =====================
# acc 0.8611111111111112
# [0.02100275 0.         0.         0.03810533 0.         0.
#  0.13964046 0.         0.         0.         0.03671069 0.3624326
#  0.40210817]
# =================== RandomForestClassifier =====================
# acc 0.9444444444444444
# [0.13789135 0.02251876 0.01336314 0.03826336 0.02830375 0.05255915
#  0.14261827 0.00916645 0.03234439 0.13563367 0.07199803 0.13963923
#  0.17570046]
# =================== GradientBoostingClassifier =====================
# acc 0.9166666666666666
# [1.43484205e-01 4.06686613e-02 6.69215300e-03 1.66979468e-03
#  1.32685947e-02 1.46219235e-08 1.13207893e-01 9.28570988e-04
#  1.26289568e-03 1.61668502e-01 3.37186677e-03 2.48779007e-01
#  2.64997841e-01]
# =================== XGBClassifier =====================
# acc 0.9444444444444444
# [0.07716953 0.03067267 0.04416747 0.00285905 0.01373686 0.0016962
#  0.07846211 0.00365221 0.02516203 0.08851561 0.00581782 0.528609
#  0.09947944]

# RandomForestClassifier와 XGBClassifier가 가장 높은 정확도를 보였고, 
# 이들 모델이 중요하다고 평가한 특성이 비슷합니다.
# 11번째 특성과 12번째 특성이 중요하다고 평가받았습니다.


# ### Dataset: 11_digits ###
# =================== DecisionTreeClassifier =====================
# acc 0.8472222222222222
# [0.         0.00886642 0.00544332 0.01521574 0.00567348 0.04850847
#  0.         0.         0.         0.0155011  0.01128834 0.00283519
#  0.01376369 0.01847882 0.00077323 0.         0.         0.00441178
#  0.00988328 0.00842401 0.04104339 0.08596583 0.00077323 0.
#  0.00103098 0.         0.07540495 0.05639249 0.04959762 0.01583598
#  0.01060644 0.         0.         0.06354131 0.00462313 0.00139182
#  0.07712915 0.03225891 0.01308039 0.         0.         0.00292356
#  0.0745241  0.05873763 0.02066762 0.00885585 0.00535316 0.
#  0.         0.00152153 0.00341824 0.00489287 0.00391772 0.01633694
#  0.02674242 0.         0.         0.         0.00115985 0.00601834
#  0.05943809 0.00201041 0.         0.00573917]
# =================== RandomForestClassifier =====================
# acc 0.9777777777777777
# [0.00000000e+00 1.46684371e-03 2.15828252e-02 8.63100042e-03
#  7.74184369e-03 2.11484828e-02 9.75321481e-03 6.78704204e-04
#  9.99062839e-05 1.05410820e-02 2.10193342e-02 7.69191223e-03
#  1.81855748e-02 2.60401050e-02 5.48039124e-03 5.48899644e-04
#  3.24578083e-05 6.79716398e-03 2.34860951e-02 2.37832862e-02
#  3.03543272e-02 5.15856136e-02 8.81822788e-03 2.96316379e-04
#  2.55452052e-05 1.23990699e-02 4.38164383e-02 2.48623827e-02
#  3.31805152e-02 2.20501425e-02 2.71615704e-02 4.56565143e-05
#  0.00000000e+00 2.85024358e-02 2.85417748e-02 1.85314325e-02
#  3.94845951e-02 2.02555900e-02 2.41710385e-02 0.00000000e+00
#  2.82560661e-05 1.08591464e-02 3.44453029e-02 4.44882149e-02
#  2.14025504e-02 1.74062193e-02 2.14496422e-02 1.37295796e-04
#  1.23540702e-04 2.39008914e-03 1.72195928e-02 2.13446288e-02
#  1.33219640e-02 2.63532308e-02 2.50370391e-02 1.21113019e-03
#  0.00000000e+00 2.03549096e-03 2.50731385e-02 1.02211980e-02
#  2.51334337e-02 2.97603141e-02 1.84862656e-02 3.28051993e-03]
# =================== GradientBoostingClassifier =====================
# acc 0.9638888888888889
# [0.00000000e+00 9.43576218e-04 8.26580828e-03 2.94656962e-03        
#  2.96477366e-03 5.89214682e-02 5.62951522e-03 4.34895132e-04        
#  8.88357015e-04 1.94699112e-03 2.10461728e-02 7.50665807e-04        
#  9.24907780e-03 7.02341958e-03 1.07147802e-03 9.52193955e-04        
#  8.50885455e-05 3.76139301e-03 1.65281821e-02 3.89863704e-02        
#  1.91841797e-02 8.88369856e-02 6.61116931e-03 2.38909959e-07        
#  1.34856142e-04 9.31860025e-04 4.67291289e-02 1.70230382e-02        
#  3.59069557e-02 2.54396527e-02 1.23327198e-02 1.21625408e-04        
#  0.00000000e+00 5.92042233e-02 4.25109611e-03 4.85830918e-03
#  7.20654432e-02 9.73895610e-03 1.60212503e-02 0.00000000e+00
#  0.00000000e+00 7.10152985e-03 7.93040245e-02 7.13175441e-02
#  9.18208302e-03 2.09890571e-02 2.88972048e-02 2.95059335e-04
#  2.73206342e-06 1.10655416e-03 5.54005762e-03 1.60042573e-02
#  7.94635927e-03 1.37117876e-02 2.80831126e-02 3.46165095e-04
#  6.15332082e-04 8.99662610e-05 1.52688645e-02 5.91413053e-04
#  5.64553453e-02 5.95652996e-03 2.15413779e-02 7.86595771e-03]
# =================== XGBClassifier =====================
# acc 0.9694444444444444
# [0.         0.03260561 0.00599625 0.00737296 0.00518664 0.04148781
#  0.00560644 0.         0.         0.00720591 0.0132025  0.00289933
#  0.01232869 0.01128461 0.00088012 0.01497376 0.         0.00730367
#  0.00837447 0.04088178 0.01003765 0.04985778 0.00374498 0.
#  0.         0.00520189 0.03477967 0.01006788 0.03358783 0.02671578
#  0.01016452 0.         0.         0.07282628 0.00650188 0.00796701
#  0.05700776 0.01347518 0.03782811 0.         0.         0.0100892
#  0.03535675 0.03744606 0.00969996 0.01911054 0.02964951 0.
#  0.         0.00542436 0.00263066 0.01368503 0.01321433 0.01782674
#  0.03249998 0.         0.         0.         0.02799876 0.00390022
#  0.06468432 0.01331835 0.03346457 0.03264587]

# andomForestClassifier가 가장 높은 정확도를 보였고,
# GradientBoostingClassifier와 XGBClassifier도 높은 정확도를 보였습니다. 
# 각 모델이 중요하게 생각하는 특성의 패턴이 다릅니다.

# 모델 비교: 정확도가 가장 높은 모델을 선택합니다. 
# 예를 들어, RandomForestClassifier가 가장 높은 정확도를 보였다면 
# 이 모델이 데이터에 잘 맞는다고 판단할 수 있습니다.

# 특성 중요도 분석: 모델이 중요하게 생각하는 특성을 분석하여, 
# 특정 특성이 결과에 얼마나 영향을 미치는지 이해할 수 있습니다. 
# 이 정보를 바탕으로 데이터의 특성을 개선하거나, 모델을 조정할 수 있습니다.

# 이렇게 결과를 해석하고 적용하면, 데이터를 잘 이해하고 문제를 해결하는 데 도움을 줄 수 있습니다.

