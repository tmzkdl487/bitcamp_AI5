import tensorflow as tf

#1. 데이터
x = [1,2,3]
y = [1,2,3]

# w = tf.Variable(111, dtype=tf.float32)
# b = tf.Variable(0, dtype=tf.float32)

w = tf.Variable(tf.random_normal([1]), dtype=tf.float32)
b = tf.Variable(tf.random_normal([1]), dtype=tf.float32)

#2. 모델 구성
# y = wx + b => y = xw + b  # 이제는 말할 수 있다.

hypothesis = x * w + b

# 3-1. 컴파일 
# model.compoile (loss='mse', optimizer='sgd')
loss = tf.reduce_mean(tf.square(hypothesis - y))    # mse

optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)   # <- Adam쓰고 싶으면 GradientDescentOptimizer을 Adam으로 바꾸면 됨.
train = optimizer.minimize(loss)

# 3-2. 훈련
# sess = tf.compat.v1.Session() # with문으로 써도 똑같음.
with tf.compat.v1.Session() as sess:
    sess.run(tf.global_variables_initializer()) # 변수 초기화

    # model.fit()
    epochs = 1001
    for step in range(epochs):
        sess.run(train)
        if step % 20 == 0:    
            print(step, sess.run(loss), sess.run(w), sess.run(b))   # <- verbose임.
# sess.close()   

# 0 0.24505009 [0.798808] [0.86935794]
# 20 0.03491117 [0.7829908] [0.49331284]
# 40 0.013190374 [0.8666096] [0.30322772]
# 60 0.0049836887 [0.9180081] [0.18638694]
# 80 0.0018829741 [0.94960153] [0.11456762]
# 100 0.0007114406 [0.9690212] [0.07042199]
# 120 0.00026879748 [0.9809581] [0.04328671]
# 140 0.000101560036 [0.98829544] [0.02660728]
# 160 3.837235e-05 [0.9928055] [0.01635488]
# 180 1.4498123e-05 [0.99557763] [0.01005292]
# 200 5.4776174e-06 [0.99728173] [0.00617933]
# 220 2.0697823e-06 [0.9983291] [0.00379828]
# 240 7.819424e-07 [0.99897295] [0.00233469]
# 260 2.9545245e-07 [0.9993687] [0.00143505]
# 280 1.1157099e-07 [0.999612] [0.00088208]
# 300 4.219072e-08 [0.9997615] [0.00054219]
# 320 1.5920497e-08 [0.99985343] [0.00033328]
# 340 6.026847e-09 [0.9999099] [0.00020487]
# 360 2.2789095e-09 [0.9999446] [0.00012591]
# 380 8.5746404e-10 [0.99996597] [7.740214e-05]
# 400 3.24088e-10 [0.999979] [4.758392e-05]
# 420 1.2329338e-10 [0.9999871] [2.9297213e-05]
# 440 4.6673183e-11 [0.99999213] [1.7972332e-05]
# 460 1.7754095e-11 [0.9999952] [1.1042296e-05]
# 480 6.915949e-12 [0.999997] [6.7825517e-06]
# 500 2.4679518e-12 [0.99999815] [4.1678945e-06]
# 520 8.7159907e-13 [0.99999887] [2.5387017e-06]
# 540 3.2684966e-13 [0.9999993] [1.5691329e-06]
# 560 1.563194e-13 [0.9999996] [9.73087e-07]
# 580 6.158037e-14 [0.99999964] [7.108268e-07]
# 600 6.158037e-14 [0.99999976] [5.8367027e-07]
# 620 3.7895614e-14 [0.99999994] [3.531989e-07]
# 640 4.7369517e-15 [0.99999994] [1.465694e-07]
# 660 0.0 [1.] [5.9149244e-08]
# 680 0.0 [1.] [5.9149244e-08]
# 700 0.0 [1.] [5.9149244e-08]
# 720 0.0 [1.] [5.9149244e-08]
# 740 0.0 [1.] [5.9149244e-08]
# 760 0.0 [1.] [5.9149244e-08]
# 780 0.0 [1.] [5.9149244e-08]
# 800 0.0 [1.] [5.9149244e-08]
# 820 0.0 [1.] [5.9149244e-08]
# 840 0.0 [1.] [5.9149244e-08]
# 860 0.0 [1.] [5.9149244e-08]
# 880 0.0 [1.] [5.9149244e-08]
# 900 0.0 [1.] [5.9149244e-08]
# 920 0.0 [1.] [5.9149244e-08]
# 940 0.0 [1.] [5.9149244e-08]
# 960 0.0 [1.] [5.9149244e-08]
# 980 0.0 [1.] [5.9149244e-08]
# 1000 0.0 [1.] [5.9149244e-08]

# 벡터 형식으로 나옴.