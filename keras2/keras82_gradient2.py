import numpy as np

f = lambda x : x**2 -4*x +6
# def f(x):
#     return x**2 -4*x +6 <- 위에랑 똑같은 뜻

gradient = lambda x : 2*x -4

x = -10.0   # 초기값
epochs = 200
learning_rate = 0.1

print('epoch \t x \t f(x)') # epoch    x       f(x)
print("{:02d}\t {:6.5f}\t {:6.5f}\t".format(0, x, f(x)))    # 00       -10.00000       146.00000

for i in range(epochs):
    x = x - learning_rate * gradient(x)
    
    print("{:02d}\t {:6.5f}\t {:6.3f}\t".format(i+1, x, f(x)))

# 01       -7.60000        94.160   
# 02       -5.68000        60.982   
# 03       -4.14400        39.749   
# 04       -2.91520        26.159   
# 05       -1.93216        17.462   
# 06       -1.14573        11.896   
# 07       -0.51658         8.333   
# 08       -0.01327         6.053   
# 09       0.38939          4.594
# 10       0.71151          3.660
# 11       0.96921          3.063
# 12       1.17537          2.680
# 13       1.34029          2.435
# 14       1.47223          2.279
# 15       1.57779          2.178
# 16       1.66223          2.114
# 17       1.72978          2.073
# 18       1.78383          2.047
# 19       1.82706          2.030
# 20       1.86165          2.019